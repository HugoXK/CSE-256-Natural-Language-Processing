{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###__CSE 256 Programming Assignment 2__\n",
        "\n",
        "Ke Xu (kex005@ucsd.edu)\n",
        "\n",
        "__Note__: To ensure the consistency and reproducibility of my experiments, all computational tasks are conducted on a __Tesla T4 GPU__ provided by the __Google Colab__ platform."
      ],
      "metadata": {
        "id": "N1McJJNPznZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs89k7sB3adl",
        "outputId": "84997cd2-9ba0-4bf3-c17f-bc07d5c35764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/CSE256PAs/PA2\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cse256/assignments/PA2/'\n",
        "FOLDERNAME = None\n",
        "FOLDERNAME = 'CSE256PAs/PA2'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# This is later used to use the IMDB reviews\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHa4pV-JxYZg",
        "outputId": "b2e0d80c-53bf-49ff-97f0-8f1b0273e47e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 14 23:19:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doyjr5Bj5ODs",
        "outputId": "30eb7052-b75b-4d07-bc73-46c8d3ab74f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUehAbpl5sCR",
        "outputId": "d9b57dde-c4ee-4725-87e7-4db15d5be926"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 PA2_code/main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WroYGv-4OqX",
        "outputId": "67711cac-62c7-418c-8383-b2fe563e4fa3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data and creating tokenizer ...\n",
            "Vocabulary size is 5755\n",
            "Total trainable parameters for encoder_model: 575187\n",
            "Epoch 1, Loss: 1.0766, Test Accuracy: 33.33%\n",
            "Epoch 2, Loss: 1.0313, Test Accuracy: 39.87%\n",
            "Epoch 3, Loss: 0.9583, Test Accuracy: 52.67%\n",
            "Epoch 4, Loss: 0.8579, Test Accuracy: 63.07%\n",
            "Epoch 5, Loss: 0.7481, Test Accuracy: 64.13%\n",
            "Epoch 6, Loss: 0.6201, Test Accuracy: 75.33%\n",
            "Epoch 7, Loss: 0.5273, Test Accuracy: 74.93%\n",
            "Epoch 8, Loss: 0.4050, Test Accuracy: 78.93%\n",
            "Epoch 9, Loss: 0.3376, Test Accuracy: 81.73%\n",
            "Epoch 10, Loss: 0.2595, Test Accuracy: 80.93%\n",
            "Epoch 11, Loss: 0.1951, Test Accuracy: 84.27%\n",
            "Epoch 12, Loss: 0.1376, Test Accuracy: 85.60%\n",
            "Epoch 13, Loss: 0.1050, Test Accuracy: 84.40%\n",
            "Epoch 14, Loss: 0.1049, Test Accuracy: 85.73%\n",
            "Epoch 15, Loss: 0.0984, Test Accuracy: 84.27%\n",
            "Input tensor shape: torch.Size([1, 32])\n",
            "Number of attention maps: 4\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Total trainable parameters for decoder_model: 942459\n",
            "Step 100 Train Perplexity: 498.0761413574219\n",
            "Step 200 Train Perplexity: 323.67205810546875\n",
            "Step 300 Train Perplexity: 211.1782684326172\n",
            "Step 400 Train Perplexity: 151.31732177734375\n",
            "Step 500 Train Perplexity: 119.29962158203125\n",
            "LM Training Loss: 5.771663032643264\n",
            "Input tensor shape: torch.Size([1, 32])\n",
            "Number of attention maps: 4\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Step 500 Obama Perplexity: 329.2257995605469\n",
            "Step 500 H. Bush Perplexity: 376.1542663574219\n",
            "Step 500 W. Bush Perplexity: 447.5278625488281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###__Part 3 Exploration__"
      ],
      "metadata": {
        "id": "p0KPr_pYdagT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cse256/assignments/PA2/'\n",
        "FOLDERNAME = None\n",
        "FOLDERNAME = 'CSE256PAs/PA2'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# This is later used to use the IMDB reviews\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppnFpBa-P6qy",
        "outputId": "868bfddc-b6f2-4340-f523-7e4a46f0cf97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/CSE256PAs/PA2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKkCTHC0P_hc",
        "outputId": "07267fdf-2a64-412a-b19a-1e51e4e0a845"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture Exploration\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "\n",
        "from PA2_code.tokenizer import SimpleTokenizer\n",
        "from PA2_code.dataset import SpeechesClassificationDataset, LanguageModelingDataset\n",
        "from PA2_code.transformer import TransformerEncoder, FeedforwardClassifier, SpeechSegmentModel\n",
        "from PA2_code.transformer import TransformerDecoderWithDisentangledAttention, TransformerDecoderWithSparseAttention\n",
        "from PA2_code.utilities import Utilities"
      ],
      "metadata": {
        "id": "sTmJXE7xeW7g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Decoder - TransformerDecoderWithSparseAttention\n",
        "\n",
        "seed = 42\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\"\"\" Hyperparameters to use for training to roughly match\n",
        "the numbers mentioned in the assignment description \"\"\"\n",
        "batch_size = 16  # Number of independent sequences  we will process in parallel\n",
        "block_size = 32  # Maximum context length for predictions\n",
        "learning_rate = 1e-3  # Learning rate for the optimizer\n",
        "n_embd = 64  # Embedding dimension\n",
        "n_head = 2  # Number of attention heads\n",
        "n_layer = 4  # Number of transformer layers\n",
        "\n",
        "\n",
        "eval_interval = 100  # How often to evaluate train and test perplexity during training\n",
        "max_iters = 500 # For language modeling, we can process all the batches for the entire dataset, but that takes a while, so we'll limit it to 500 iterations. For batch size of 16 and block size of  32, this is roughly, this is  500 * 16 * 32 = 256000 tokens, SOTA LMs are trained on trillions of tokens, so this is a very small dataset.\n",
        "eval_iters = 200  # Number of iterations to evaluate perplexity on the test set\n",
        "\n",
        "\n",
        "## classifier training hyperparameters. It is a simple 1 hidden layer feedforward network, with input\n",
        "## size of 64, hidden size of 50 and output size of 3.\n",
        "\n",
        "n_input = 64  # Input size for the classifier, should match the embedding size of the transformer\n",
        "n_hidden = 100  # Hidden size for the classifier\n",
        "n_output = 3  # Output size for the classifier, we have 3 classes\n",
        "epochs_CLS = 15 # epochs for classifier training\n",
        "\n",
        "def load_texts(directory):\n",
        "    \"\"\"\n",
        "    This function loads all texts from the specified directory, ignoring any files with \"test\" in their name. The text is used for \"training\" the tokenizer. Since our tokenizer is simple, we don't need to do any training, but we still need to ignore the test data.\n",
        "    \"\"\"\n",
        "\n",
        "    texts = []\n",
        "    files = os.listdir(directory)\n",
        "    for filename in files:\n",
        "        if \"test\" in filename:  ## don't \"read test files\"\n",
        "            continue\n",
        "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
        "            texts.append(file.read())\n",
        "    return texts\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\" Collate a batch of data into a single tensor with padding.\"\"\"\n",
        "    data, labels = zip(*batch)  # Separate the data and labels\n",
        "    # Pad sequences to the fixed length\n",
        "    padded_sequences = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "    padded_sequences = padded_sequences[:, :block_size]  # Truncate if longer\n",
        "    # Add padding if shorter\n",
        "    padded_sequences = torch.nn.functional.pad(padded_sequences, (0, max(0, block_size - padded_sequences.shape[1])), \"constant\", 0)\n",
        "    labels = torch.stack(labels)\n",
        "    return padded_sequences, labels\n",
        "\n",
        "def compute_perplexity(decoderLMmodel, data_loader, criterion, eval_iters=100):\n",
        "    \"\"\" Compute the perplexity of the decoderLMmodel on the data in data_loader.\n",
        "    Make sure to use the cross entropy loss for the decoderLMmodel.\n",
        "    \"\"\"\n",
        "    decoderLMmodel.eval()\n",
        "    losses= []\n",
        "    with torch.no_grad():\n",
        "        for X, Y in data_loader:\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            outputs = decoderLMmodel(X) # your model should be computing the cross entropy loss\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), Y.view(-1))\n",
        "            losses.append(loss.item())\n",
        "            if len(losses) >= eval_iters: break\n",
        "\n",
        "    losses = torch.tensor(losses)\n",
        "    mean_loss = losses.mean()\n",
        "    perplexity = torch.exp(mean_loss).item()  # Calculate perplexity as exp(mean loss)\n",
        "\n",
        "    decoderLMmodel.train()\n",
        "    return perplexity\n",
        "\n",
        "def main():\n",
        "\n",
        "    print(\"Loading data and creating tokenizer ...\")\n",
        "    texts = load_texts('speechesdataset')\n",
        "    tokenizer = SimpleTokenizer(' '.join(texts)) # create a tokenizer from the data\n",
        "    print(\"Vocabulary size is\", tokenizer.vocab_size)\n",
        "\n",
        "    inputfile = \"speechesdataset/train_LM.txt\"\n",
        "    inputfile_obama = \"speechesdataset/test_LM_obama.txt\"\n",
        "    inputfile_wbush = \"speechesdataset/test_LM_wbush.txt\"\n",
        "    inputfile_hbush = \"speechesdataset/test_LM_hbush.txt\"\n",
        "\n",
        "    with open(inputfile, 'r', encoding='utf-8') as f:\n",
        "        lmtrainText = f.read()\n",
        "    train_LM_dataset = LanguageModelingDataset(tokenizer, lmtrainText,  block_size)\n",
        "    train_LM_loader = DataLoader(train_LM_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    with open(inputfile_obama, 'r', encoding='utf-8') as f:\n",
        "        obamatestText = f.read()\n",
        "    test_obama_dataset = LanguageModelingDataset(tokenizer, obamatestText,  block_size)\n",
        "    test_obama_loader = DataLoader(test_obama_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    with open(inputfile_wbush, 'r', encoding='utf-8') as f:\n",
        "        wbushtestText = f.read()\n",
        "    test_wbush_dataset = LanguageModelingDataset(tokenizer, wbushtestText,  block_size)\n",
        "    test_wbush_loader = DataLoader(test_wbush_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    with open(inputfile_hbush, 'r', encoding='utf-8') as f:\n",
        "        hbushtestText = f.read()\n",
        "    test_hbush_dataset = LanguageModelingDataset(tokenizer, hbushtestText,  block_size)\n",
        "    test_hbush_loader = DataLoader(test_hbush_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # for the language modeling task, you will iterate over the training data for a fixed number of iterations like this:\n",
        "    decoder_model = TransformerDecoderWithSparseAttention(n_layer, n_embd, n_head, tokenizer.vocab_size).to(device)\n",
        "    print(\"####### Decoder: TransformerDecoderWithSparseAttention #######\")\n",
        "    num_decoder_parameters = sum(p.numel() for p in decoder_model.parameters())\n",
        "    print(f\"Total trainable parameters for decoder_model: {num_decoder_parameters}\")\n",
        "    optimizer = optim.Adam(decoder_model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    decoder_model.train()\n",
        "    for i, (xb, yb) in enumerate(train_LM_loader):\n",
        "        if i >= max_iters:\n",
        "            break\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        # LM training code here\n",
        "        optimizer.zero_grad()           # Reset gradients to zero for each batch\n",
        "        outputs = decoder_model(xb)      # Forward pass\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "        yb = yb.view(-1)\n",
        "        loss = criterion(outputs, yb)    # Compute loss\n",
        "        loss.backward()                 # Backpropagate the loss\n",
        "        optimizer.step()                # Update the model parameters\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Step {i + 1} Train Perplexity: {compute_perplexity(decoder_model, train_LM_loader, criterion)}\")\n",
        "\n",
        "    print(f\"Step 500 Obama Perplexity: {compute_perplexity(decoder_model, test_obama_loader, criterion)}\")\n",
        "    print(f\"Step 500 H. Bush Perplexity: {compute_perplexity(decoder_model, test_hbush_loader, criterion)}\")\n",
        "    print(f\"Step 500 W. Bush Perplexity: {compute_perplexity(decoder_model, test_wbush_loader, criterion)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI10HQZoI9xd",
        "outputId": "92809171-574c-41a3-cb91-49f1d35b5e0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data and creating tokenizer ...\n",
            "Vocabulary size is 5755\n",
            "####### Decoder: TransformerDecoderWithSparseAttention #######\n",
            "Total trainable parameters for decoder_model: 975227\n",
            "Step 100 Train Perplexity: 319.5060119628906\n",
            "Step 200 Train Perplexity: 137.37399291992188\n",
            "Step 300 Train Perplexity: 75.67100524902344\n",
            "Step 400 Train Perplexity: 44.10769271850586\n",
            "Step 500 Train Perplexity: 29.1240177154541\n",
            "Step 500 Obama Perplexity: 120.5716781616211\n",
            "Step 500 H. Bush Perplexity: 137.09829711914062\n",
            "Step 500 W. Bush Perplexity: 166.67526245117188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cse256/assignments/PA2/'\n",
        "FOLDERNAME = None\n",
        "FOLDERNAME = 'CSE256PAs/PA2'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# This is later used to use the IMDB reviews\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPIjita4eEsr",
        "outputId": "453acbe8-d96f-4555-d525-23ef4b939aec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/CSE256PAs/PA2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Improvement - Learning Rate Scheduler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "\n",
        "from PA2_code.tokenizer import SimpleTokenizer\n",
        "from PA2_code.dataset import SpeechesClassificationDataset, LanguageModelingDataset\n",
        "from PA2_code.transformer import TransformerEncoder, FeedforwardClassifier, SpeechSegmentModel\n",
        "from PA2_code.transformer import SpeechSegmentModel, TransformerDecoder\n",
        "from PA2_code.utilities import Utilities\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "wlAy4wqDdZ4u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seed = 42\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\"\"\" Hyperparameters to use for training to roughly match\n",
        "the numbers mentioned in the assignment description \"\"\"\n",
        "batch_size = 16  # Number of independent sequences  we will process in parallel\n",
        "block_size = 32  # Maximum context length for predictions\n",
        "learning_rate = 1e-3  # Learning rate for the optimizer\n",
        "n_embd = 64  # Embedding dimension\n",
        "n_head = 2  # Number of attention heads\n",
        "n_layer = 4  # Number of transformer layers\n",
        "\n",
        "\n",
        "eval_interval = 100  # How often to evaluate train and test perplexity during training\n",
        "max_iters = 500 # For language modeling, we can process all the batches for the entire dataset, but that takes a while, so we'll limit it to 500 iterations. For batch size of 16 and block size of  32, this is roughly, this is  500 * 16 * 32 = 256000 tokens, SOTA LMs are trained on trillions of tokens, so this is a very small dataset.\n",
        "eval_iters = 200  # Number of iterations to evaluate perplexity on the test set\n",
        "\n",
        "\n",
        "## classifier training hyperparameters. It is a simple 1 hidden layer feedforward network, with input\n",
        "## size of 64, hidden size of 50 and output size of 3.\n",
        "\n",
        "n_input = 64  # Input size for the classifier, should match the embedding size of the transformer\n",
        "n_hidden = 100  # Hidden size for the classifier\n",
        "n_output = 3  # Output size for the classifier, we have 3 classes\n",
        "epochs_CLS = 15 # epochs for classifier training\n",
        "\n",
        "def load_texts(directory):\n",
        "    \"\"\"\n",
        "    This function loads all texts from the specified directory, ignoring any files with \"test\" in their name. The text is used for \"training\" the tokenizer. Since our tokenizer is simple, we don't need to do any training, but we still need to ignore the test data.\n",
        "    \"\"\"\n",
        "\n",
        "    texts = []\n",
        "    files = os.listdir(directory)\n",
        "    for filename in files:\n",
        "        if \"test\" in filename:  ## don't \"read test files\"\n",
        "            continue\n",
        "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
        "            texts.append(file.read())\n",
        "    return texts\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\" Collate a batch of data into a single tensor with padding.\"\"\"\n",
        "    data, labels = zip(*batch)  # Separate the data and labels\n",
        "    # Pad sequences to the fixed length\n",
        "    padded_sequences = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "    padded_sequences = padded_sequences[:, :block_size]  # Truncate if longer\n",
        "    # Add padding if shorter\n",
        "    padded_sequences = torch.nn.functional.pad(padded_sequences, (0, max(0, block_size - padded_sequences.shape[1])), \"constant\", 0)\n",
        "    labels = torch.stack(labels)\n",
        "    return padded_sequences, labels\n",
        "\n",
        "def compute_classifier_accuracy(classifier, data_loader):\n",
        "    \"\"\" Compute the accuracy of the classifier on the data in data_loader.\"\"\"\n",
        "    classifier.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for X, Y in data_loader:\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            outputs = classifier(X)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_correct += (predicted == Y).sum().item()\n",
        "            total_samples += Y.size(0)\n",
        "        accuracy = (100 * total_correct / total_samples)\n",
        "        classifier.train()\n",
        "        return accuracy\n",
        "\n",
        "import math\n",
        "\n",
        "def main():\n",
        "    print(\"Loading data and creating tokenizer ...\")\n",
        "    texts = load_texts('speechesdataset')\n",
        "    tokenizer = SimpleTokenizer(' '.join(texts)) # create a tokenizer from the data\n",
        "    print(\"Vocabulary size is\", tokenizer.vocab_size)\n",
        "\n",
        "    train_CLS_dataset = SpeechesClassificationDataset(tokenizer, \"speechesdataset/train_CLS.tsv\")\n",
        "    train_CLS_loader = DataLoader(train_CLS_dataset, batch_size=batch_size, collate_fn=collate_batch, shuffle=True)\n",
        "    test_CLS_dataset = SpeechesClassificationDataset(tokenizer, \"speechesdataset/test_CLS.tsv\")\n",
        "    test_CLS_loader = DataLoader(test_CLS_dataset, batch_size=batch_size, collate_fn=collate_batch, shuffle=True)\n",
        "\n",
        "    # Model\n",
        "    encoder = TransformerEncoder(n_embd=n_embd, n_head=n_head, n_layer=n_layer, vocab_size=tokenizer.vocab_size)\n",
        "    classifier = FeedforwardClassifier(n_input=n_input, n_hidden=n_hidden, n_output=n_output)\n",
        "    encoder_model = SpeechSegmentModel(encoder, classifier).to(device)\n",
        "\n",
        "    num_encoder_parameters = sum(p.numel() for p in encoder_model.parameters())\n",
        "    print(f\"Total trainable parameters for encoder_model: {num_encoder_parameters}\")\n",
        "\n",
        "    # Optimizer and loss function\n",
        "    optimizer = optim.Adam(encoder_model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define the Cosine Annealing learning rate scheduler\n",
        "    print(\"##### Learning Rate Scheduler #####\")\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs_CLS, eta_min=1e-6)\n",
        "\n",
        "    encoder_model.train()\n",
        "    for epoch in range(epochs_CLS):\n",
        "        total_loss = 0\n",
        "        for xb, yb in train_CLS_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()  # Reset gradients to zero for each batch\n",
        "            output = encoder_model(xb)  # Forward pass\n",
        "            loss = criterion(output, yb)  # Compute loss\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "            optimizer.step()  # Update the model parameters\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Step through the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate average loss and test accuracy for the epoch\n",
        "        average_loss = total_loss / len(train_CLS_loader)\n",
        "        test_accuracy = compute_classifier_accuracy(encoder_model, test_CLS_loader)\n",
        "        print(f'Epoch {epoch + 1}, Loss: {average_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5Gq5OuWdV5h",
        "outputId": "9f145540-99c0-4c61-9004-2553c464e86f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data and creating tokenizer ...\n",
            "Vocabulary size is 5755\n",
            "Total trainable parameters for encoder_model: 575187\n",
            "Epoch 1, Loss: 1.0787, Test Accuracy: 33.33%, Learning Rate: 0.000989\n",
            "Epoch 2, Loss: 1.0542, Test Accuracy: 44.93%, Learning Rate: 0.000957\n",
            "Epoch 3, Loss: 1.0026, Test Accuracy: 49.87%, Learning Rate: 0.000905\n",
            "Epoch 4, Loss: 0.9114, Test Accuracy: 50.40%, Learning Rate: 0.000835\n",
            "Epoch 5, Loss: 0.7922, Test Accuracy: 63.20%, Learning Rate: 0.000750\n",
            "Epoch 6, Loss: 0.6751, Test Accuracy: 69.87%, Learning Rate: 0.000655\n",
            "Epoch 7, Loss: 0.5311, Test Accuracy: 77.33%, Learning Rate: 0.000553\n",
            "Epoch 8, Loss: 0.3927, Test Accuracy: 79.87%, Learning Rate: 0.000448\n",
            "Epoch 9, Loss: 0.3020, Test Accuracy: 80.80%, Learning Rate: 0.000346\n",
            "Epoch 10, Loss: 0.2324, Test Accuracy: 83.33%, Learning Rate: 0.000251\n",
            "Epoch 11, Loss: 0.1807, Test Accuracy: 84.40%, Learning Rate: 0.000166\n",
            "Epoch 12, Loss: 0.1338, Test Accuracy: 85.07%, Learning Rate: 0.000096\n",
            "Epoch 13, Loss: 0.1108, Test Accuracy: 85.33%, Learning Rate: 0.000044\n",
            "Epoch 14, Loss: 0.0997, Test Accuracy: 85.20%, Learning Rate: 0.000012\n",
            "Epoch 15, Loss: 0.0937, Test Accuracy: 85.33%, Learning Rate: 0.000001\n"
          ]
        }
      ]
    }
  ]
}